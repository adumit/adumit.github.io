# ReZero is All You Need: Fast Convergence at Large Depth
Link: https://arxiv.org/pdf/2003.04887.pdf

This seems to be a somewhat lesser known method, but it has been referenced in a few high profile papers. The ease with which one can implement the method combined with it's ability to speed up training makes it a great tool to have.

## Goal
Improve convergence rates for networks by taking advantage of a mathematical result about dynamical isometry from another paper about how to achieve more efficient learning.

## Method
![Rezero image 1]({{site.url}}/assets/images/linformer/image-1.png)

The method is stupid-simple, really. It's just a residual layer, but with a learned parameter that starts at 0!

## How does it perform?
### Fully connected networks
![Rezero image 2]({{site.url}}/assets/images/linformer/image-2.png)
When training fully connected networks, the training loss converges enormously more quickly. 7 - 15x faster! The larger the network the more valuable ReZero becomes. So, they naturally trained a 10,000 layer network on a laptop with 1 GPU to prove it. They do successfully overfit the training set without diverging.

### ConvNets
![Rezero image 3]({{site.url}}/assets/images/linformer/image-3.png)
On convnets, the network does quite a bit better as well. Looking to the depth of the network, we again see that ReZero improves relatively further with the depth of the network. 

### Transformers
![Rezero image 4]({{site.url}}/assets/images/linformer/image-4.png)
Typically, transformers have a complicated warmup step and require a good bit of finetuning. They show that with ReZero, they can achieve faster convergence to a reference value and also achieve close to SoTA with a vanilla transformer augmented with the ReZero initialization.

![Rezero image 5]({{site.url}}/assets/images/linformer/image-5.png)
Their method also avoids the need for LayerNorm in transformers, which is a convenient addition. This change should save memory and compute!

#### What is being learned?
![Rezero image 6]({{site.url}}/assets/images/linformer/image-6.png)
The authors look at the alpha values learned over training. The deepest layers tend to learn a higher alpha value right away and then all layers decay towards 0, ultimately learning a very small residual.

### Does it work in practice?
This change was so easy to implement, that I went and tried it for a problem I was working on. It appeared to speed up training up to a point, but then the typical residual layers did catch up. I was operating in a fully feed forward manner.

However, the use of ReZero resulted in improved generalization for my problem! The accuracy on the validation set improved by ~2%, which is a significant increase for the project. The truly amazing thing about this result is how easy it is to test for yourself and verify. I will definitely be testing it in new projects.

Additionally, another area this kind of method is seeing use in is for integrating new modalities of data. In the Flamingo model by DeepMind, they used a ReZero initialization - just the alpha parameter that governs the residual - for integrating the visual attention values into the multi-modal network. This let the network smoothly transition from knowing nothing about video and images to fully integrating them over the course of training.