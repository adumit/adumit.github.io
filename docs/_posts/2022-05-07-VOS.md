---
layout: post
title:  "Paper read: VOS - Learning what you don't know by virtual outlier synthesis"
date:   2022-05-07 15:00:00 +0200
categories: paper-reads
---

# VOS - Learning what you don't know by virtual outlier synthesis

Link to paper: https://arxiv.org/pdf/2202.01197v3.pdf

### Goal of the paper

Out-of-distribution detection (OOD) is an especially important problem to solve because it allows for safer deployments of neural networks. The authors try and improve detection by synthesizing outliers rather than relying on an a-priori constructed outlier dataset. This allows them to regularize the model's decision boundaries during training!

### Methods
#### What is the method they are using/improving upon?

The author's goal is to synthesize outliers ahead of test time to regularize the decision boundaries of the model.

![Figure 1]({{site.url}}/assets/images/VOS-post-figure-1.png)

Figure 1 shows an example of an ideal boundary. The three grey clusters of points are different classes sampled from a mixture of gaussians. In figure 1b, we can see one possible decision boundary where the model is only uncertain of points that lie between the three clusters. The authors show that if you train with virtual outliers, you get much tighter decision boundaries.

##### What is their primary method?

![Figure 2]({{site.url}}/assets/images/VOS-post-figure-2.png)

Figure 2 shows the framework of VOS. Their method starts with the base model that creates a feature vector for each input. Then, they use those feature vectors to generate virtual outliers and train a classification head and a 2-class logistic regression head.

To create virtual outliers, they sample via the following equation:

![Figure 2b]({{site.url}}/assets/images/VOS-post-figure-2b.png)

During training, the algorithm is computed as follows:

![Figure 3a]({{site.url}}/assets/images/VOS-post-figure-3.png)

During training, we use equations 1 and 2 to compute the class centers and covariances:

![Figure 4]({{site.url}}/assets/images/VOS-post-figure-4.png)

The algorithm maintains a queue of examples for each class from the batches it seems during training to compute the above.

It then uses equation (3) from above to sample virtual outliers and then can compute the uncertainty loss for the OOD classifier in equation (5) below:

![Figure 5]({{site.url}}/assets/images/VOS-post-figure-5.png)

The final loss is a combination of classification loss, localization loss, and the uncertainty loss: 

![Figure 6]({{site.url}}/assets/images/VOS-post-figure-6.png)

Then, at prediction time, we can compute an uncertainty score and use threshold that to predict whether this example is OOD. The OOD uncertainty score is computed as follows:

![Figure 7]({{site.url}}/assets/images/VOS-post-figure-7.png)

And then we simply threshold it:

![Figure 8]({{site.url}}/assets/images/VOS-post-figure-8.png)

The authors state that we typically choose a gamma such that a high fraction of ID data (>95%) is correctly classified as in-distribution.

#### What is the main advance in their method?
- Their method enables adaptive outlier synthesis.
- Their method does not require a separate outlier dataset, which could be costly or impossible to determine for some datasets.
- Compared to other outlier synthesis methods, their method is more advantageous compared to GANs or random noise directly in the high-dimensional pixel space.

### Results
#### What are the main results they use to justify their work?

![Figure 9]({{site.url}}/assets/images/VOS-post-figure-9.png)

Table 1 is the primary table for their work. FPR95 is false-positive-rate at 95% is the false positive rate at 95% true positive rate. It's a standard metric for out-of-distribution assessment. Essentially, if we want to correctly classify 95% of the in-distribution examples, how likely are we to get a false positive.

The other major piece of evidence they use, which does not have a table (probably because they cannot highlight their method?), is comparison to outlier exposure (OE), which directly trains on an outlier dataset. In that example, they achieve 88.66% AUROC compared to 90.18% AUROC from OE. This seems like a solid outcome. Essentially, if you can collect outliers and know what outliers look like, then that method may be for you.

#### Are these results convincing?
The results in the table are fairly convincing. Their method seems to be quite a bit stronger than the other methods. The main point of consideration here is what assumptions do the other methods make? VOS makes a fairly strong assumption (see next section) about the form of the data. If the data they train on contains this assumption as well, then it might simply be that their method is a better match for the data rather than being generally a better method.

#### Are there any limitations of the method?
The method assumes the data it is operating on is composed of a mixture of gaussians. This assumption of normality certainly does not hold for many datasets. Further, their method seems that it could apply generally to deep-networks, but they only test it on vision networks. Testing on other modalities would have made their results a lot stronger.

### Discussion
#### Where could this be applied?
I'm curious if this works for other kinds of tasks and modalities beyond object detection. When I was looking for the definition of FPR95, there are examples of running outlier detection on CIFAR-100. It would probably be even more compelling if the authors tested their method in additional settings.

#### What kinds of problems would this work well for?
Unclear if this method's dominance is specific to object detection and localization. I'm very interested to see how this might apply to situation for which you do not actually know what outliers look like. In cases where you do not know what outliers look like, synthesizing outliers could then prove to be a strong solution for ensuring the network is not overconfident in unseen areas. It might be a way to expose where the model is uncertain given the current set of data.
