---
layout: post
title:  "Paper read: Stochastic Weight Averaging"
date:   2022-05-08 10:00:00 +0200
categories: paper-reads
---

# Stochastic Weight Averaging

Paper link: https://arxiv.org/pdf/1803.05407.pdf

### Claims:

1.  Better generalization
2.  Flatter loss landscapes    
3.  Low computational overhead
4.  Approximates Fast Geometric Ensembling (FGE)
    
### Background:

-   FGE is a method to ensemble slight variations of the same neural network.
-   Same architecture, slightly different weights
-   FGE does this in model space
-   Weight space is opposed to in model space, which is averaging the outputs of a network. This is a traditional ensembling method.
    

### The algorithm:

-   Always starts with a pretrained model in the paper.
-   In the cold-start case, you’d probably train your model to its optimum and then train for a few more epochs and run SWA.
-   Uses a cyclical learning rate or high constant learning rate.
-   This is to encourage different models and to get out of the optimum value found in the previous iteration.
-   “We note the name SWA has two meanings: on the one hand, it is an average of SGD weights. On the other hand, with a cyclical or high constant learning rate, SGD proposals are approximately sampling from the loss surface of the DNN,leading to stochastic weights.”

![Image 1]({{site.url}}/assets/images/SWA/Image-1.png)

### Figure 1: The weight averaging along test vs. train error loss landscapes 

![Image 2]({{site.url}}/assets/images/SWA/Image-2.png)

On figure 1:
-   This is showing taking the average of 3 weight iterations.
-   What this accomplishes is moving slightly away from the very lowest point of training loss (dark red) and into a nearby and a nearly equivalent region (orange).
-   This adjustment supposedly helps ensure generalization by moving towards the wider optima value in orange rather than the spikey optimum in dark red.

### Figure 2: The cyclical learning rate used

![Image 3]({{site.url}}/assets/images/SWA/Image-3.png)

We can see that the learning rate adjusts linearly and cyclically over each epoch and the test error rate spikes and then approaches an optimum. Each optimum is in theory (and practically, an almost certainty given the complex loss landscape) a different point in weight space even though the output model space could be somewhat similar.
-   They use this abrupt cycling procedure to encourage exploration, which is more important to this procedure than the accuracy at each point.

![Image 4]({{site.url}}/assets/images/SWA/Image-4.png)

In figure 3, each image is a point in the loss landscape that is either the beginning of an epoch, the mid-point of an epoch or the termination of an epoch. This demonstrates that the model explores many points around the optimum value in test space, but does not hit it.

### What does solution width have to do with it?

-   Prior work claims that the width of the local optima in the training landscape determines the generalization of the network.
-   Geometrically, this is saying that the wider loss landscape means the model will perform approximately the same even with small perturbations. This is good for generalization.
    
### Solution width demonstrated

![Image 5]({{site.url}}/assets/images/SWA/Image-5.png)

In figures 4 and 5, the authors show that the solution width (in model space) is wider using SWA than with SGD. This is done by starting at the optimum solution found by the method and then taking some random step along a ray in N-dimensional space (where N is weight dimensionality) and evaluating the train and test loss. 
-   Basically, this is adjusting the weights very slightly and seeing what happens to the error.

In figure 4, we should recognize that we have to move much further away from our weights in order to induce a significant change in the test error rate!

In figure 5, we should recognize that the SWA version achieves a lower test error rate even if training error rate is slightly higher. Further, the authors claim this is a general feature of training and test curves.
-   Related - in prior work, it was argued that the loss landscape around SGD solutions with large batches is very sharp. This factor can be remedied by using SWA, which finds a flatter region.
    
### Compared to ensembling formally

-   Theoretically, the authors compare the two and conclude that the outcome values between the two are small.
-   The difference in the final outputs can be thought about as F(avg(w)) vs. avg(F(w)). Since F is a nonlinear function, these two computations are unequal. However, due to the fact that both FGE and SWA encourage weight values that are not too far apart, the small perturbations result in small differences.
-   Empirically, they evaluate the two variants by comparing the two models on the same dataset.
-   They conclude that FGE and SWA are very close in the output probabilities. Norm value of 0.079, which is actually smaller than comparing between two FGE proposals.
-   Further, the two variants output the same label 95.26% of the time, which is, again, closer than successive FGE proposals.
    
### Accuracy improvements

![Image 6]({{site.url}}/assets/images/SWA/Image-6.png)

![Image 7]({{site.url}}/assets/images/SWA/Image-7.png)

Across the board improvements.

### Effect of the learning rate scheduler

![Image 8]({{site.url}}/assets/images/SWA/Image-8.png)

-   Seems to work for most learning rates and across constant and a cyclical LR.    
-   Higher learning rates (but not too high!) are better to encourage exploration.
-   However, there’s no clear trend in what works best.
    
### Experiment with the cold-start problem

![Image 9]({{site.url}}/assets/images/SWA/Image-9.png)

-   They run SGD for 300-epochs on CIFAR-100 with Wide-ResNet-28-10 from random initialization and after 140 epochs they begin averaging the weights at test time.
-   They show that the model quickly converges to a good value.

### Questions I was thinking about:
- For models without a pre-trained version, should we use SWA once the model has converged on a test error? Or, should we run SWA even while we are still learning?
- In my experience, experimentally, it was effective to run SWA once the model was in a relative region of test error (e.g. 1-2% difference) rather than waiting until final convergence.
